# pytorch搭建网络（定义模型）
（1）导入`torch.nn`模块，nn就是利用autograd定义模型，nn的核心数据结构是`Module`，这是一个抽象概念，可以表示神经网络中某个词或者包含多层的神经网络。
实际运用时，最常见做法是继承nn.Module，一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法
## 这部分以后我得好好理清楚面向对象的部分
个人理解：nn.Module就是标准积木，现在我们的模型就是某个我们设计好的模型玩具，要想搭成这个玩具，我们先得从标准积木（nn.Module）中构建出我们要的指定模块(网络层)，最后forward方法就是自己搞出来的‘拼装手册’，怎么把我们的积木拼成我们的玩具。

### 想想8x8微型积木就好了
![](http://ys-d.ys168.com/614621432/813403427/k64523X6JGVNWFlpglqo59/1610940088437.jpg)

当然，简单方法就是nn.Sequential，这个最方便，直接net=nn.Sequential（网络层），网络层按你传入的顺序添加到计算图中
可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。
```python
for param in net.parameters():
    print(param)
```

# 给不同子网络设置不同学习率（finetune常用）
举例：
```python
optimizer =optim.SGD([
                # 如果对某个参数不指定学习率，就使用最外层的默认学习率
                {'params': net.subnet1.parameters()}, # lr=0.03
                {'params': net.subnet2.parameters(), 'lr': 0.01}
            ], lr=0.03)
```
    似乎在pytorch中，也有类似的方法用于将多个子网络训练，之后可以进行研究

# torch中的各种轮子（深度学习框架就是这么用的嘛）
torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。

# Softmax特点
softmax运算常被用在分类问题，他的好处就是将我们多分类的输出值变换成正值且和为一的概率分布
之后我们可以通过取最大概率作为分类判断依据
参考网页：<https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_342-softmax%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8b>


























