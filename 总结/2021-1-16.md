# tensor和numpy转换
使用numpy()和from_numpy()相互转换,这两函数生成结果和源数据是共享内存的(速度快,但是改变一个另一个也改变)

另一方法是torch.tensor(),该方法是先数据拷贝,时间空间消耗大,也不共享内存

除了CharTensor外,所有在CPU上的tensor都可以和numpy数组转换
(感想:自由啊,回想起tensorflow中连print都打印不出内容的情况,pytorch某种程度上方便太多了!)

用to()将tensor在GPU,CPU上移动,当然你得有gpu
指定device: `device = torch.device('cuda')`
移动: `x=x.to(device)`
这就把实现了一次移动

# autograd
这个包能根据输入和前向传播自动构建计算图和执行反向传播
核心类:tensor
将属性`.requires_grad`设置`True`就会追踪`tensor`的操作(那么就可以利用链式法则进行梯度传播),调用`.backward()`完成梯度计算,`tensor`的梯度会积累到`.grad`属性中
如果要取消追踪,就要调用.`detach()`从追踪记录分离,也可以使用`with torch.no_grad()`将不想追踪的代码包裹起来,这在评估模型(eval)时很有用

补充资料:PyTorch 的 backward 为什么有一个 grad_variables 参数？
<https://zhuanlan.zhihu.com/p/29923090>

    我觉得这部分我看的是挺头晕的,这部分的逻辑还是要再深入看看的

# 什么是超参数
需要强调的是，这里的批量大小和学习率的值是**人为设定的**，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。**我们通常所说的“调参”指的正是调节超参数**，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。
来源:<https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3-%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95>
    之后有机会可以详细写一篇关于超参数的整合文章

# 计算代码运行时间
`start = time()`
`应该运行的代码块`
`print(time() - start)`

# python的yield关键字
起因是看到这样一段代码：
来源是：<https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.2_linear-regression-scratch?id=_322-%e8%af%bb%e5%8f%96%e6%95%b0%e6%8d%ae>

```python
# 本函数已保存在d2lzh包中方便以后使用
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # 样本的读取顺序是随机的
    for i in range(0, num_examples, batch_size):
        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch
        yield  features.index_select(0, j), labels.index_select(0, j)
```
这是一个简单的dataloader函数，但是在最后返回值上，我们可以看到他并没使用return，而是用了yield关键字

关于yield，得讨论到生成器generator，他是记录一定的算法规则，和一般的迭代器不同的是，他是在需要调用时再根据算法推算出相应元素，而不用一口气算完
yield类似return，回想一下return，函数中执行到这个关键字就会返回响应结果并终止函数执行

当yield被塞入函数里，会发生什么呢->该函数变成了生成器->我们能用next使用它或遍历

考虑到生成器的目的：记录一定算法规则得到按需生成的列表
所以yield一般在for循环语句以一定规则生成，之后调用函数，得到一个生成器，再遍历生成器使用它

## 当然，yield也可以用在别的地方，生成器只是一种使用方法

`就像打电玩一样，你蓄力发大招的时候，如果执行了return，就直接把大招发出去了，蓄力结束
如果执行了yield，就相当于返回一个生成器对象，每调用一次next（），就发一个大招`（来源知乎）

与return不同，yield在函数中返回值时会保存函数的状态，使下一次调用函数时会从上一次的状态继续执行，即从yield的下一条语句开始执行，这样做有许多好处，比如我们想要生成一个数列，若该数列的存储空间太大，而我们仅仅需要访问前面几个元素，那么yield就派上用场了，它实现了这种一边循环一边计算的机制，节省了存储空间，提高了运行效率。

总结整理我们最开始的这段代码：
我们包装的这么一个dataloader，应该将我们的数据根据需要的batchsize分成一段一段的数据（如1000数据，bs=4，就是250组），然后每次输出一组数据，这时候根据yield会保存到该组数据位置的状态，然后该组进入网络运算过程，实现网络和反向传播后，再循环回来找dataloader要下一组数据，yield的功能就体现出来了，因为我们从直接的状态就可以直接读下一组而不是从头按照索引找新的一组数据。这样

注：在pytorch中，我们的dataloader库也使用了类似这样的方法

# 














