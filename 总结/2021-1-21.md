# 分开定义softmax运算和交叉熵损失函数可能会造成数值不稳定
摘自:<https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.7_softmax-regression-pytorch?id=_373-softmax%e5%92%8c%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>

## 暂未解决,后续查资料了解下

先写下softmax函数的公式,假设我们有一数组V,$V_{i}$表示V中第i个元素,那么该元素的softmax值是:
$$S_{i}=\frac{e^{i}}{\sum_{j}^{}e^{j} } $$

其中原mxnet版本,沐神给的问题是:
```
3.6.10. 练习
- 在本节中，我们直接按照softmax运算的数学定义来实现softmax函数。这可能会造成什么问题？（提示：试一试计算 exp(50) 的大小。）
- 本节中的cross_entropy函数是按照“softmax回归”一节中的交叉熵损失函数的数学定义实现的。这样的实现方式可能有什么问题？（提示：思考一下对数函数的定义域。）
- 你能想到哪些办法来解决上面的两个问题？
```
第一个问题很明显,exp(50)数值计算溢出(即x较大),为了解决这个问题的方法就是改变x的值咯,搬运知乎一份回答:<https://zhuanlan.zhihu.com/p/27223959>
![](http://ys-o.ys168.com/614621441/813405382/krcmuns453633755MJa1/softmax%E4%BC%98%E5%8C%96.png)
公式变换:
```python
def softmax(X):
    X = X-X.max()
    exp_X = X.exp()
    return exp_X/exp_X.sum()
```
即通过将每个$V_{i}$值减去数组V中最大值来起到降低exp()函数输入的作用,解决问题

第二个问题先看这个交叉熵函数,这里摘录动手学深度学习pytorch版本2.5节的代码,并非mxnet版本
```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat.gather(1, y.view(-1, 1)))
#其中数据为:
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y = torch.LongTensor([0, 2])
```
y.view(-1,1)使用了pytorch中的torch.view()函数,用来将y变换形态,其中的参数-1代表这个位置数值由其他值推断,所以应该得到
```python
print(y.view(-1,1))

tensor([[0],
        [2]])
```
理一下上面的,y_hat是两个数据对应3个类别的概率图,y是两个数据对应的类别标签,所以我们用gather取出对应类别的概率,输出为:
```python
tensor([[0.1000],
        [0.5000]])
```
参考<https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_345-%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>
写的交叉熵函数,此处的$y_{j}$非1即0,所以我们的函数就被简化成$-log(\hat{y}_{i})$了

思考对数定义域为(0, -&),但是因为输出$\hat{y}$是softmax输出不应该存在负数(指数函数),应考虑点是如果$\hat{y}$输出过小时无限接近于0,该交叉熵函数即负对数函数结果值过大为nan
可以考虑在交叉熵中添加一极小值避免情况

以上全文参考文献:
[请问练习里的这两个问题有什么解决方案？我实在没想到，因为第一个exp(50)好大好大，第二个问题，ln定义域必须大于0，但根本不可能小于0啊，因为损失函数出来的值根本不会小于等于0啊](https://discuss.gluon.ai/t/topic/741/171)


# 对于分开定义不稳定的解决方法
虽然这句话有点nt,我们用一个包括两者的函数即可,数值稳定性会更好
pytorch中,这个函数称为:`nn.CrossEntropyLoss()`
该损失函数结合了nn.LogSoftmax()和nn.NLLLoss()两个函数,它在做分类（具体几类）训练的时候是非常有用的。
[torch官方文档:CROSSENTROPYLOSS](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

# 激活函数,yyds
从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。
上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。

摘自:<https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.8_mlp?id=_382-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>

# CS231n讲解权重初始化
[CS231n课程笔记翻译：神经网络笔记 2](https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit)

# 如何理解训练误差和泛化误差(一个极好的例子,沐神666)
摘录链接:[3.11.1 训练误差和泛化误差](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.11_underfit-overfit?id=_3111-%e8%ae%ad%e7%bb%83%e8%af%af%e5%b7%ae%e5%92%8c%e6%b3%9b%e5%8c%96%e8%af%af%e5%b7%ae)

在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。

让我们以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。假设训练题和测试题都随机采样于一个未知的依照相同考纲的巨大试题库。如果让一名未学习中学知识的小学生去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成一名反复练习训练题的高三备考生答题，即使在训练题上做到了错误率为0，也不代表真实的高考成绩会如此。

在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。基于该独立同分布假设，给定任意一个机器学习模型（含参数），它的训练误差的期望和泛化误差都是一样的。例如，如果我们将模型参数设成随机值（小学生），那么训练误差和泛化误差会非常相近。但我们从前面几节中已经了解到，模型的参数是通过在训练数据集上训练模型而学习出的，参数的选择依据了最小化训练误差（高三备考生）。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。

机器学习模型应关注降低泛化误差。

# csv数据预处理
来源是<动手学>的房价预测,一直在做图像处理,还没怎么见过csv格式的处理,学习一下
[3.16.3 预处理数据](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.16_kaggle-house-price?id=_3163-%e9%a2%84%e5%a4%84%e7%90%86%e6%95%b0%e6%8d%ae)

# 



















