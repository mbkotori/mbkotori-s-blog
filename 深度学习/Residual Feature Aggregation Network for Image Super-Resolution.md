翻译名:用于图像超分辨的残差特征聚合网络

abstract:
解决问题-SISR,单张图像超分辨率问题
考虑点-利用残差特征中的 残差分支-分层特征( However, existing methods neglect to fully utilize the hierarchical features on the residual branches.)
提出解决方法-提出残差特征聚合(residual feature aggregation,RFA)框架,将残差块组合在一起,并添加跳过链接[有点像densenet啊这个说法和图];提出增强空间注意力特征(enhanced spatial attention,ESA),使残差更集中于空间特征(?)
最后将RFA和ESA一同运用

introduction:
SISR-直接将低分辨率LR图转成高分辨率HR图(单进单出siso),不像一般超分辨会有ms图

{读图figure1}
这里(a)实际上是一个多resblock的前向结构,每个模块包含一个resblock,由四个resblock组成
(b)中描述他们的RFA结构,先通过3个resblock前向图,他们的输出到第四个resblock,之后Res4模块输出和前三个block各自的输出concat到一块,随后1x1卷积来聚集/融合他们的特征,相对于(a)增加了1x1卷积的参数消耗

{读图figure8}
不同残差块输出的特征可以反映空间内容的不同方面
|--考虑思路:用空间注意力来提升空间信息表达能力,增强特征的空间分布

自称工作:
1.提出RFA框架
2.提出加强空间注意力ESA块
3.提出RFANet,基于RFA和ESA模块进行构建

realated work
sub-pixel conv:子像素卷积,超分辨率中常用的upscale方法
https://oldpan.me/archives/upsample-convolve-efficient-sub-pixel-convolutional-layers
[后续可能会出这篇文章的内容,我还没读但是看起来很有意思,他和deconv方法的不同也可以考虑进去,另一点是他能应用在SR上能不能应用在分割上,做一下考虑]

{读图figure2}
基础的SR模型,卷积+多个设计好的模块+反卷积+卷积回复通道数?

基于注意力的网络模型

3.methodology
3.1 SR的简单网络结构
看作三部分:
头部:负责一个卷积层的初始特征提取,得到浅层特征F0
躯干:输入是提取到的特征F0,连接到T个basemodule的复读模式{公式2}
重建restore:将上面提取的(深层特征Ft和浅层特征F0)融合,进行放大[全局残差学习可以减轻训练难度]
upsample模块是重建中的关键部分,使用子像素卷积

损失函数用L1 loss(改损失函数或许是一个很好的改进切入点)

3.2 残差特征聚合模块
残差模块包含双分支(1)残差块(2)身份块?(identity branch)
前面的残差块提取的信息很难被利用->因为它需要很长路径到达最后的卷积运算
->改进[直接将每个块的残差特征concat到一起]
->concat后用1x1卷积融合他们的特征哦(这个东西可不仅仅是降维减通道)

这样的连接,不用让前面的残差信息受损或干扰->得到更具有区分性的特征表达(?)

RFA可以看作一个独立模块,这也意味着我们可以把它和其他模块连接(模块复读和不同模块组合,哪个效果会好一些呢)

3.3 增强的空间注意力模块
{读图figure4}
左侧是ESAblock的形状,右侧是ESAblock中的ESA机制(mechanism)
仔细看ESA机制,对其做一个解释

ESA块放到残差块的末尾来起作用,注意力机制本质(改天可以再写一篇文章),集中体现残差特征
同时考虑到该ESA块要被连接到你后面的每个残差块,尽量不要做得太臃肿
为了完成图像SR任务,应设计一个具有较大感受野的模块(?此处没有理解.关于感受野下次可以再写一篇)

解决方法也很简单:1x1卷积降维减参数+步长卷积增大感受野
注意这里,描述的是strided conv(步长卷积)而不是dilated conv,这两者存在区别(可以详细写一下)

跨步卷积搭配池化使用#为什么/原因.可以详细描述一下

**这里提到的:"抛去计算量考虑,实现空间注意力块的更好方法是使用非本地模块(Non-local block)"我没get到这个部分,之后要详细看一下他这里参考的两篇论文**


3.4 实施细节
把RFA和ESA一同使用来构建最终的SR模块(RFANet):
|-使用30个RFA模块,每个RFA包含4个ESA模块(从这里就感受到了前文为什么他们说要做一个轻便的ESA模块了)
这里的四个ESA模块也很明显:在每个resblock后,我们都给他插入一个ESA(真的有效吗....保持怀疑)
此处提到的1x1卷积的缩小率(这是个啥?),减速比? reduction ratio

3.5 讨论discussions
将文章网络和MemNet、RDN进行了讨论,描述了他们的优越性

**4 实验内容**
4.1 设置参数,训练等
数据集来源:DIV2K数据集
数据增强:随机旋转+翻转

batchsize=16,patch_size=48*48
数据集:Set5,Set14,B100,Urban100,Manga109

*Bicubic(BI) and blur-downscale (BD) degradation models [36] are
used when conducting experiments.*(这部分是什么意思?没有理会到)

论文评价：
就我个人而言，这篇文章并没有太大的吸引力：RFA模块类似DenseNet结构，但是又没有很好的去解决密集连接后的参数爆炸（更有甚者他在原网络基础上又增加了ESA模块，就算通过设计使增加的参数不至于过多也是实打实地在每个模块上又添了“浓墨重彩”的一笔支出）。看似巧妙但是仔细琢磨似乎只是对网络的二度修改尝试。
另一点上，消融实验的可解释性确实不高，反而给出了一些漏洞，作者试图用实验解释自己制造的模块的作用，但是弄巧成拙，并没有取得很好的结果。
当然，作为CVPR2020录用的文章，从中我们还是能学习到一些东西的：比如文章中图表制作的合理性（我很喜欢为消融实验所做的table1和计算效率&参数量的figure9，两张图既表达清晰又给人眼前一亮）；还有其中ESA模块的方法，在不能合理解释的情况下尝试他也未尝不可（作者也利用这个模块#######）